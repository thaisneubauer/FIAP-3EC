{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "\n",
    "## Case: Lending Club\n",
    "\n",
    "Mais um conjunto de dados do Kaggle. O desafio pode ser visto [aqui](https://www.kaggle.com/wendykan/lending-club-loan-data). Lá, estão disponíveis várias das soluções (Kernels) realizadas para esse desafio. \n",
    "\n",
    "O Lending Club (ou Clube de empréstimos, em tradução livre) é uma empresa de empréstimo peer-to-peer com sede nos Estados Unidos, na qual os investidores fornecem fundos para potenciais tomadores de empréstimos e os investidores obtêm um lucro, dependendo do risco que correm (a pontuação de crédito do tomador). O Lending Club faz a \"ponte\" entre investidores e quem precisa de empréstimo.\n",
    "\n",
    "<img src=\"../images/loan.png\" width = 600/>\n",
    "\n",
    "\n",
    "Os dados que vamos trabalhar neste notebook contém dados completos de empréstimos realizados entre 2007-2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos dados\n",
    "\n",
    "O arquivo de dados original (que está lá no Kaggle), tem 1.2Gb. Fiz um filtro para usarmos somente as linhas referentes a empréstimos que já foram liquidados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:06:37.717734Z",
     "start_time": "2020-03-23T23:06:37.711120Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_complete = pd.read_csv('loan_complete.csv')\n",
    "#df = df_complete[(df_complete.loan_status != 'Current')]\n",
    "#df.to_csv(loan.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda assim, o arquivo tem 800Mb (e 329 se comprimirmos em um zip) e o GitHub só aceita arquivos de até 50M, pois o objetivo central do Git é armazenar o histórico de CÓDIGOS! \n",
    "\n",
    "O tamanho dos dados ser superior a esse limite é extremamente comum e por isso, daqui para frente vou manter dados no Google Drive e inserir na aula o download deles.\n",
    "\n",
    "O arquivo dos dados que trabalharemos neste notebook tem 329Mb e com essa biblioteca do Google Drive `google_drive_downloader`, conseguimos acompanhar o progresso do download com o parâmetro `showsize = True`.\n",
    "\n",
    "<!--- !pip install googledrivedownloader ---> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:06:38.034465Z",
     "start_time": "2020-03-23T23:06:37.729339Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "def download_dataset(dataset_path, google_id):\n",
    "    if not os.path.exists(dataset_path):\n",
    "        gdd.download_file_from_google_drive(file_id=google_id,\n",
    "                                            dest_path=dataset_path, \n",
    "                                            showsize = True)\n",
    "        ''\n",
    "def unzip_dataset(dataset_path, unzipped_dataset_path):\n",
    "    if not os.path.exists(unzipped_dataset_path):\n",
    "        print(\"Unzipping ...\")\n",
    "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('../data/')\n",
    "        print(\"Done!\")\n",
    "\n",
    "dataset_path = '../data/loan.zip'\n",
    "google_id = '1GrXYO6WjPAMwb1czTtgsdwU0_VtYBuKQ'\n",
    "download_dataset(dataset_path, google_id)\n",
    "\n",
    "unzipped_dataset_path = '../data/loan.csv'\n",
    "unzip_dataset(dataset_path, unzipped_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes de começarmos: análise exploratória\n",
    "\n",
    "A análise exploratória, pelo menos uma básica, temos que fazer SEMPRE que nos depararmos com um novo conjunto de dados. Vamos novamente usar as operações que aprendemos para entender um pouco do contexto desses dados, principalmente as colunas que queremos selecionar e as características que queremos/podemos extrair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T01:42:26.701729Z",
     "start_time": "2020-03-24T01:42:26.016890Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#maximo de colunas\n",
    "pd.set_option(\"display.max_rows\",150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T01:42:34.963865Z",
     "start_time": "2020-03-24T01:42:26.703958Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/loan.csv', nrows = 200000, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada na lista das colunas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos mais de 1 milhão de tomadores de empréstimos nessa base, ou seja, cada linha (instância) temos as características (features) de uma pessoa que pediu empréstimo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:31.894526Z",
     "start_time": "2020-03-23T23:07:31.882066Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:31.601909Z",
     "start_time": "2020-03-23T23:07:31.573130Z"
    }
   },
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercício:</b>\n",
    "Carreguem o arquivo \"loan_metadata.xlsx\" e explorem suas colunas e os significados delas. \n",
    "Selecionem duas listas de colunas que julgarem serem úteis no cenário de previsão de um empréstimo ser pago ou não: a primeira com colunas numéricas e a segunda com colunas não-numéricas.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features numéricas\n",
    "Num primeiro momento, vamos selecionar do nosso conjunto algumas *features* (colunas) numéricas, pois comumente, modelos de *machine learning* só aceitam esse tipo de dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:31.905467Z",
     "start_time": "2020-03-23T23:07:31.897246Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_vars = ['loan_amnt', 'int_rate', 'installment', 'annual_inc',\n",
    "       'dti', 'delinq_2yrs', 'inq_last_6mths',\n",
    "       'mths_since_last_delinq', 'mths_since_last_record', 'open_acc',\n",
    "       'pub_rec', 'revol_bal', 'revol_util', 'total_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:32.048371Z",
     "start_time": "2020-03-23T23:07:31.915734Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df = df[initial_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a feature é numérica, posso simplesmente usar? \n",
    "\n",
    "- Exemplo:\n",
    "\n",
    "<img src=\"../images/exe.png\" alt=\"exemplo\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização e normalização\n",
    "\n",
    "![standardization](../images/standardization.jpg)\n",
    "\n",
    "- Padronização ou regularização: uma adequação bastante usada quando lidamos com features numéricas é mapear os valores de uma distribuição para valores de uma distribuição normal padrão para que, independentemente dos valores que temos na distruição, tenhamos a mesma **grandeza** de valores.\n",
    "- Distribuição normal padrão: uma distribuição normal que tem média 0 e desvio padrão 1.\n",
    "- Cálculo: para realizar esse mapeamento, calcula-se a diferença entre cada um dos valores e a média e dividimos essa diferença pelo desvio padrão.\n",
    "\n",
    "<img src=\"../images/standardization-calc.jpg\" alt=\"calculo_padronizacao\" width=\"400\"/>\n",
    "\n",
    "- Exemplo: \n",
    "\n",
    "![standardization_exe](../images/standardization-exe.jpg)\n",
    "\n",
    "- **Normalização**: tem como objetivo colocar os valores da distribuição dentro do intervalo de 0 e 1 ou, caso tenha resultado negativo, entre -1 e 1.\n",
    "\n",
    "<img src=\"../images/norm-calc.png\" alt=\"calculo_norm\" width=\"200\"/>\n",
    "\n",
    "\n",
    "#### Atenção!!!\n",
    "Se a distribuição não é normal (Gaussiana) ou se o desvio padrão é muito pequeno, a melhor escolha é **normalizar**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padronização/normalização no Python e o Scikit-learn\n",
    "\n",
    "O pacote [Scikit-learn](https://scikit-learn.org/stable/) (`sklearn`) é uma biblioteca Python para Machine Learning.\n",
    "Vamos usar **bastante** essa biblioteca. Ela tem muita coisa de ML pronta (e bem feita) e uma documentação exemplar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:32.387596Z",
     "start_time": "2020-03-23T23:07:32.061066Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voltando aos nossos dados numéricos, podemos usar a padronização do Scikit-learn...\n",
    "\n",
    "Mas antes, **precisamos substituir os nulos**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:32.503234Z",
     "start_time": "2020-03-23T23:07:32.389788Z"
    }
   },
   "outputs": [],
   "source": [
    "#verificando a porcentagem de nulos por coluna\n",
    "numeric_df.isnull().mean().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer a substituição pela média ou mediana, por exemplo.\n",
    "\n",
    "\\* Uma forma um pouco mais robusta: média condicionada (selecionar grupos de dados com base em alguma característica e substituir pela média dentro do grupo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T00:46:47.142110Z",
     "start_time": "2020-03-24T00:46:47.129512Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercício:</b>\n",
    "Calcule a média e a mediana das colunas numéricas selecionadas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T00:46:47.142110Z",
     "start_time": "2020-03-24T00:46:47.129512Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercício:</b>\n",
    "Preencha os valores nulos das colunas com a média ou a mediana.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:33.331481Z",
     "start_time": "2020-03-23T23:07:33.028652Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, podemos aplicar a padronização usando o [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) do Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:35.166504Z",
     "start_time": "2020-03-23T23:07:33.333904Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:35.233433Z",
     "start_time": "2020-03-23T23:07:35.171132Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:35.314988Z",
     "start_time": "2020-03-23T23:07:35.262827Z"
    }
   },
   "outputs": [],
   "source": [
    "#os scalers do Scikit-learn retornam listas numpy como resposta\n",
    "#se quisermos acessar esses dados em forma de Dataframe, precisamos fazer a conversão:\n",
    "numeric_df_scaled = pd.DataFrame(scaled_data, columns = numeric_df.columns)\n",
    "numeric_df_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem outros padronizadores/normalizadores, como o [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), com o qual conseguimos fazer a normalização comentada acima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features categóricas\n",
    "\n",
    "Em vários algoritmos e modelos de IA, só são aceitos valores numéricos. Então precisamos de uma forma de mapear (*encoders*) as categorias para números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:35.332830Z",
     "start_time": "2020-03-23T23:07:35.327877Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = ['term', 'grade', 'emp_length', 'home_ownership', 'application_type', \n",
    "            'verification_status', 'pymnt_plan', 'purpose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder\n",
    "\n",
    "Cada valor único da feature categórica é mapeado para um ID único. No Scikit-learn: [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercício:</b>\n",
    "Com a documentação do LabelEncoder do Scikit-learn, aplique a função \"fit_transform\" na coluna categórica \"term\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#preencha aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é necessário preencher os nulos para que esse *encoder* funcione, mas podemos preencher com uma string vazia para que todas as linhas para as quais essa informação não existe receba um mesmo ID. Vamos aplicar o encoder para todas as colunas categóricas e já preencher os nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:38.810511Z",
     "start_time": "2020-03-23T23:07:35.341130Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "for col in cat_cols:\n",
    "    new_df[col] = df[col].fillna('')\n",
    "    new_df[col] = None\n",
    "\n",
    "select_df = pd.concat([numeric_df, new_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:39.230425Z",
     "start_time": "2020-03-23T23:07:38.812881Z"
    }
   },
   "outputs": [],
   "source": [
    "select_df[cat_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T22:08:05.502852Z",
     "start_time": "2020-03-23T22:08:05.482567Z"
    }
   },
   "source": [
    "**Atenção:** \n",
    "- Os IDs são numéricos e, portanto, **implicam uma ordem**! \n",
    "- Por isso, só pode ser usado com algoritmos não lineares baseados em árvore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "- Usado com a maioria dos algoritmos lineares\n",
    "- Podemos excluir a primeira coluna evita colinearidade\n",
    "- Não introduz ordem ao atributo\n",
    "- Aumenta a **dimensionalidade** do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:39.378221Z",
     "start_time": "2020-03-23T23:07:39.233880Z"
    }
   },
   "outputs": [],
   "source": [
    "select_df[cat_cols].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) do Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:40.845488Z",
     "start_time": "2020-03-23T23:07:39.383795Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "for col in cat_cols: \n",
    "    col_dummies = pd.get_dummies(df[col], drop_first=True, prefix = col)\n",
    "    new_df = pd.concat([new_df, col_dummies], axis = 1)\n",
    "\n",
    "select_df = pd.concat([numeric_df, new_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:40.933230Z",
     "start_time": "2020-03-23T23:07:40.847779Z"
    }
   },
   "outputs": [],
   "source": [
    "select_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros enconders\n",
    "\n",
    "#### Effect Coding\n",
    "- Bem similar ao *dummy*;\n",
    "- Substitui por -1 todos os 0s que representam uma das categorias (primeira ou última) no dummy encoding;\n",
    "- Exemplo nesse [link](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63).\n",
    "\n",
    "- [Outro exemplo](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-effect-coding/).\n",
    "\n",
    "#### Bin-counting \n",
    "- Maldição da dimensionalidade, categorias \"grandes\" (com muitos valores distintos);\n",
    "- Usa probabilidade baseada em informação estatística da relação do valor com o target;\n",
    "- Exemplo simplificado do [link 2](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63): construir valores de probabilidade de um ataque DDoS ser causado por qualquer um dos endereços IP baseado em **dados históricos** de endereços IP relacionados a ataques DDoS.\n",
    "\n",
    "#### Feature Hashing Scheme\n",
    "- Maldição da dimensionalidade;\n",
    "- **Função Hash** tipicamente usada com um número pré-definido de features resultantes (vetor de comprimento pré-definido) de forma que os valores de hash dos valores da categoria sejam usados como índices nesse vetor predefinido e os valores sejam atualizados de acordo.\n",
    "- Funções Hash mapeiam uma grande quantidade de valores em uma quantidade finita (e pequena) de valores -> colisões.\n",
    "- [Explicação básica sobre funções hash](https://www.techtudo.com.br/artigos/noticia/2012/07/o-que-e-hash.html)\n",
    "- [Função Scikit Learn](https://scikit-learn.org/0.19/modules/generated/sklearn.feature_extraction.FeatureHasher.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras práticas para CRIAÇÃO de features\n",
    "\n",
    "- A partir de qualquer tipo de feature (numérica ou categórica), podemos criar outras features, tanto derivando informações (ex: a partir da taxa de juros anual derivar a taxa mensal) quanto enriquecendo o conjunto de dados (ex: cruzando as informações que eu tenho no meu conjunto de dados para trazer novas informações de outro conjunto).\n",
    "\n",
    "- Um exemplo é quando há valores muito raros e próximos de valores mais numerosos, pois podemos agrupar esses valores por ter maior interesse em um grupo - ex: público alvo por idade.\n",
    "\n",
    "- Outro exemplo bastante comum é usar **dados geográficos** para criar features. Um exemplo no nosso case: mapear a coluna do estado (`add_state`) para uma coluna de região (que dei o nome de `region`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T23:07:41.032984Z",
     "start_time": "2020-03-23T23:07:40.946535Z"
    }
   },
   "outputs": [],
   "source": [
    "df['addr_state'].unique()\n",
    "\n",
    "west = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']\n",
    "south_west = ['AZ', 'TX', 'NM', 'OK']\n",
    "south_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\n",
    "mid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\n",
    "north_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n",
    "\n",
    "df['region'] = np.nan\n",
    "\n",
    "def finding_regions(state):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enriquecimento de dados geográficos: podemos agregar mais informação a partir dos dados geográficos que temos no conjunto de dados. [Nesse link](https://medium.com/creditas-tech/incrementando-dados-geogr%C3%A1ficos-com-o-censo-nacional-do-ibge-54d342c4bdcf) é exemplificado essa técnica com dados do IBGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material para aprofundamento:\n",
    "Para mais detalhes sobre outros métodos de Feature Engineering veja [esses slides](https://www.dropbox.com/s/mebc9i6kxgm516c/feature-engineering-mlmeetup.pdf?dl=0) do HJ van Veen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
